---
layout: post
title:  "데이터 파이프라인 만들기 (2)"
subtitle:   "Data Pipelining Camp 2nd"
categories: data
tags: pipeline
comments: false
---
```
Fast campus 의
[APACHE SPARK을 활용한 데이터 파이프라인 만들기 CAMP]
두번째 강의를 듣고 정리한 내용입니다.
```


#### 개요

오늘의 목표는 AWS에 있는 서비스를 이용해서 S3에 수집하는것이다.<br>
Kinesis Stream 을 사용하겠다.<br>

```
* 스트림과 큐의 차이점?
큐는 담아놨다가 누군가 가져가면 없어진다.
스트림은 강물처럼 흘러가는 데이터이고 없어지지 않는다.
설정한 양만큼 계속 가지고 있으면서 과거 데이터부터 없앤다.
컨슈머를 여러개 붙여도, 여러개 컨슈머에서 활용할수 있다.
```

Kinesis Stream을 쓰는 이유는 데이터를 붓고 리얼 분석을 할수도 있고, 그 밖에 여러 컨슈머를 갖다가 쓸수 있기 때문이다.<br>
온프레미스에서 사용하는 Kafka를 잘 녹여놓은 서비스이니, 비교해서 이해하면 되겠다.<br>
<br>
Api gateway, Kinesis Stream, Firehose, S3를 사용하며, 이렇게 수집한다는 것을 이해하면 된다.<br>
이벤트가 들어오게 하기 위해 프로그램을 따로 작성해 놓은 것을 사용하겠다.<br>
(csv 파일을 Api gateway 에다가 쏘는 프로그램)<br>
<br>
우리의 목표는 수집<br>
데이터 생성 된것을 out -  수집 쪽으로 in 되는데, 이런식으로 연결을 잘 시키고 확인하자.<br>
<br>
데이터를 쌓을때, 용량 제한 없는곳에 쌓는게 좋다. 왜냐하면 문제 발생시마다 계속 조정해 줘야하는 이슈가 생기기 때문이다.<br>
이슈로 인해 문제가 생성되는 부분에 대해 유연성있게 되는 시스템을 만들기 위해 애썼다. 한번 만들어놓으면 서비스에만 신경쓰게끔 하는 점을 고려한 것이다. 이렇게 해놓으면 장기적으로 쓰는데 문제가 없고, 여러시스템에 호환성도 있다.<br>
<br>
수집의 노드는 심플할수록 좋다.<br>
요즘 많이 사용하는 json 반정형 데이터로 저장 하겠다.<br>

<br>
<hr/>
<br>

#### Api gateway

이벤트를 받는 문이라고 생각하면 된다.<br>
AWS 서비스에 액세스할 수 있는 일관된 RESTful 애플리케이션 프로그래밍 인터페이스(API)를 제공한다. 서버를 따로 작성해서 띄우지 않아도 간편하게 데이터를 받을수 있게 해준다.(서버가 없는 작은 서비스에서 특히 유용할 것 같다)<br>

<br>
<hr/>
<br>

#### Kinesis Stream

Kafka와 구조가 비슷하다.<br>
Shard라고 되어있는건 Kafka의 Broker와 비슷하다.<br>
데이터량이 많으면 샤드수를 늘려줘야한다.<br>
샤드는 병렬처리를 해준다. 하나의 샤드가 처리할 수 있는 리미트가 있고, 정확한것은 찾아보면 나온다.
1분 단위의 리미트가 있다.<br>
실제 사용해본 수치를 말해보자면, 압축된 파일로 800메가(풀면 몇기가) 이정도는 샤드 하나정도면 된다.<br>
그런데 전에 말했던것처럼 마케팅이나 이벤트성으로 푸시를 보내면 트래픽이 5배정도 뛴다. 시스템은 MAX값에 맞춰야 해서 샤드 2개정도로 돌린다.<br>
One service one monitoring - CloudWatch 를 이용해서, 시스템을 감시해서, 몰릴때 자동으로 늘리게 할수 있다.<br>
소비하는 놈들(Consumer)이 연결하는 서비스들이 예로 쭉 나와있는데,<br>
* DynamoDB(MongoDB랑 똑같다)
* Redshift(클러스터링되는 RDB)
* EMR(분석하는 툴)
* Firehose
이런식으로 쭉 연결이 된다.

<br>
<hr/>
<br>

#### Firehose

실시간 스트리밍 데이터를 아래 목적지들로 전달하기 위한 관리형 서비스이다.<br>
(Serverless로 이쪽으로 보내줘 보내줘 하면 바로 딜리버리 다 한다)<br>

* S3
* Redshift
* ES (ElasticSearch)
* splunk : 로그를 갖다 부으면 차트를 촥촥보여줌 ㅋㅋ

이중에서 우리는 S3로 전달하는것만 실습하겠다.<br>
만약 S3말고 다른 목적지라면 RAW데이터는 무조건 S3에 저장하도록 레코멘드 하고 있다.<br>


<br>
<hr/>
<br>

#### S3

데이터를 저장하는 저장소이다.<br>
관련 용어를 보자.<br>
* 버킷
S3에 저장된 객체에 대한 컨테이너. 모든 객체는 어떤 버킷에 포함된다.<br>
쉽게 말하면, Root폴더라고 이해하면 되겠다.<br>
* 객체
S3에 저장되는 기본 개체이다. 객체는 객체데이터와 메타데이터로 구성된다.<br>
* 키
버킷 내 객체의 고유한 식별자.<br>
버킷 내 모든 객체는 정확히 하나의 키를 갖는다.<br>
<br>
버킷, 키 및 버전ID의 조합이 각 객체를 고유하게 식별한다.<br>

<br>
<hr/>
<br>

##### 요약!!

> 1. 데이터 생성 : mobile app / Web / 협력사<br>
> 2. Post방식으로 발송<br>
> 3. AWS로 들어오는 Api gateway를 통해서<br>
> 4. JSON 데이터를<br>
> 5. Kinesis Stream으로<br>
> 6. S3에 넣는것<br>
><br>
> 여기까지 실습한다.<br>
