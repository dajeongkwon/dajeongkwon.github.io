---
layout: post
title:  "데이터 파이프라인 만들기 (1)"
subtitle:   "Data Pipelining Camp 1st"
categories: data
tags: pipeline
comments: true
---
```
Fast campus 의
[APACHE SPARK을 활용한 데이터 파이프라인 만들기 CAMP]
첫번째 강의를 듣고 정리한 내용입니다.
```


#### 개요

데이터 분석하는데 쓰이는 도구들 이름을 들어본 적이 있을 것이다.<br>
Hadoop, Spark, ELK(ElasticSearch, Logstash, Kibana), Zeppelin 등등..<br>
Fast campus 내에도 이 각각을 다루는 강의가 있는걸 보면 알겠지만, 하나하나 깊이가 다 있다.<br>
우리는 전체 파이프라인을 배운다. 배워야 할 요소가 많아서 당연히 생소한 용어가 있을것이다.<br>

* 데이터 파이프라인 : 수집된 데이터를 각 팀에 공유를 할수 있도록 제공해주는것
* 온프레미스: IDC에 서버 넣어두고 쓰는 방식

온프레미스로 가면 이런 어려움이 있는데, 클라우드로 가면 이런 편의성과 비용적인 이점이 있다. 이런것들에 대해 설명하겠다.<br>

데이터 파이프라인 흐름을 일단 이해하고,<br>
이쪽에는 오픈소스가 무궁무진하게 많다. 그 내용중에서 중요한 것들을 공유할까 했는데, <br>
아무래도 시간이 제한되어 있으니 사용중인 AWS 패키지 요소에 대해 설명을 포커싱하는게 좋겠다.<br>

Logstash와 Kafka를 이용해서 설정해서 어떻게 흘러가는지 이해하자. <br>
온프레미스에서 데이터 수집에는 Kafka를 가장 대중적으로 많이 쓰고 있다.<br>
약간 비교해서 설명을 할 것이다. <br>
AWS 패키지가 그냥 나온게 아니라, 오픈소스에서 잘 되는 것을 가져다가 서비스로 녹여서 자기들이 서비스를 하고 거기에 따른 사용료를 받는 것이다.<br>

<br>
<hr/>
<br>

#### 데이터 파이프라인 흐름의 이해

보통 데이터 파이프라인의 시작은 이렇다. <br>
데이터 수집을 어떤 방식으로, 어떤 형식으로, 어떤 요소들을 할지 결정을 하는것부터 시작을 한다.<br>
```
수집 - 전처리/저장 - 시각화/분석
```
이러한 스텝을 거치게 되는데 그것보다 중요한 것이 있다.<br>
각 부서에서 요구사항이 많은데, <br>
요구사항을 듣고 그 요소에 대해 수집을 하고 + 그 외에도 필요할 만한것들을 수집<br>
그 중에서 어떤것을 남길지 (데이터 저장하는 양) 선정하는 과정<br>
이런게 정말 중요한데, 이걸 됐다고 보고 그 다음 스텝들을 진행하겠다.<br>

```
- 수집
- 전처리를 어떤 방식으로 어떻게 할지
- 저장을 어떻게 해야 좋을지
- 그것을 시각화랑 어떻게 연결할지
(시각화는 요구사항을 주셨던 분들이 해야할 부분. 그래서 연결을 해주면 됨)
```

<br>
<hr/>
<br>

#### 데이터 레이크(Lake)

데이터 웨어하우스와의 차이?<br>
수집해서 다 모아서 시각화해서 공유하는것 이런것은 같다.<br>
근본적인 성격이 다르다. <br>
전처리 / 시각화 다 해서 부서에 공유를 했는데 인사이트를 보다가 다른 요소가 더 도움이 될것 같아보인다 싶으면,<br>
데이터 레이크는 비정형 원천 데이터들을 그대로 다 쌓아놓기 때문에, 다시 수집되어있는 데이터를 꺼내서, 요소를 다시 적용해서 분석할수 있다.<br>
데이터 웨어하우스는 그렇지 않다. 한번 다시 하려면 처음부터 다 다시 해야한다.<br>

그렇지만 데이터 레이크가 웨어하우스를 대체하는 구조는 아니다. 서로 상생하는 구조다.<br>

<br>
<hr/>
<br>

#### 데이터 파이프라인을 구성하는 목적

**1. 요구사항에 대해서 빠른 대응을 한다**<br>
		데이터에서 이상 징후가 발생했을때 더 깊이 보고 회의한다<br>
		하루나 몇시간 내로 제공을 해주고 있다<br>
		그런 부분이 필요하다. 대표님들은 항상 급하시기에.<br>
**2. 지속적이고 에러가 없어야 한다**<br>
		보통 새벽에 배치가 돈다<br>
		대표님 회의할때 에러나있으면 안된다<br>
**3. Scability 해야한다**<br>
    데이터를 수집하다보면 데이터량이 폭주하는 경우가 있다.<br>
		푸시 발송하면 평소대비 수집되는 양이 4~5배 뛴다.<br>
		앞단이나, 저장공간 부족이라든지, 뭔가 이상이 생기기 마련이다.<br>
    이전에는 DB가 죽거나 그러면 장비를 업그레이드를 하거나, 디스크 늘려줘야 되고 이런 작업들을 했었다.<br>
		클라우드를 쓰면 시스템적으로 늘렸다 줄였다 할수있어서 손이 덜간다.<br>
		새벽에 배치 돌릴때 서버를 20대로 늘렸다가, 아침엔 4~5대로 줄이고 이런 구조로 운영 가능하다<br>
		서비스 오픈할때, 자동적으로 Scale out 하거나 그렇게 할수 있다<br>
**4. 데이터 쌓이는 공간에 문제가 없어야 한다**<br>
		기준을 폭주할때 기준으로 맞추곤 한다. toc를 잡을때. 이런걸 어떻게 해결하면 좋겠나. <br>
**5. 신규로 요구가 들어온다. 추가되는 Biz System 에 대해 빠르게 적용 가능해야 한다**<br>
    로그성 데이터 뿐 아니라, 마케팅 이벤트를 한다고 했을 경우,<br>
		발생한 이벤트를 받으면, 마케팅 채널에 어느정도로 잘되고 있는지 대시보드 만들게 되는데<br>
		* 이벤트를 할때마다 새로 만들어야 하는가
		* 그걸 빨리빨리 해주면 팀에도 도움이 될거고, 비용도 아낄수 있다
**6. 데이터 포맷 유연성있게 해야 한다**<br>
		요구사항은 나중에 계속 추가도 이루어 질수 있는거고, 줄어들수도 있다.<br>
		얼마나 능동적으로 대응할수 있는 시스템이냐.<br>
		파이프라인을 구성할 때 이런걸 고민했다.<br>
		웹 사용과 앱 사용은 다르다.<br>
		웹은 한번 배포하면 다 똑같이 일률적으로 맞출수 있는데,<br>
		앱은 버전마다 다 다른데 다 수용 해줘야 한다. 어떤 버전은 분석 안됩니다. 라는 말은 통하지 않는다.<br>

<br>
<hr/>
<br>

#### 어떤 툴들이 있나?

BigData Landscape 라고 해서, 현재 데이터 분석에서 쓰고 있는 것들이고, 매년 나온다.<br>
종류가 많다. 이중에서 사례를 많이 봐서 선택을 해야 한다.<br>
대기업은 자체 클라우드를 구성하거나 하고 있다.<br>

<br>

**Spark**<br>
그래도 보통 많이 쓰는게, 모든 파이프라인에 Spark 는 거의 들어간다. 필수적으로 쓰고 있다.<br>
기능이 뛰어나고, 클라우드에서 영업할때는 쓰라는 말 잘 안한다. <br>
클라우드에서 스파크를 적용하게 되면 클라우드 회사 입장에서 보면 돈이 별로 안되니까. <br>
우리 입장에서는 아껴서 분석을 해야하고, 그렇기 때문에 Spark를 넣는게 맞다.<br>

**Cloud Service**<br>
Google은 약간 자유롭다. 딱 들어가면, 자유롭게 개발을 해놔서 정리가 잘 안되어있다. <br>
Azure는 클라우드가 정말 윈도 같다. 보기에 좋은데, 기능은 좀 부족하게 되어있다. <br>
왜 Amazon은 깔끔하게 되어있을까, 위에서 딱 정했겠지.<br>
첫 화면 딱 보면 내가 하고 싶은 영역별로 요소가 나뉘어있다. 사용자 입장에서는 그런게 필요한것.<br>

**ElasticSearch**<br>
핫한것은 ElasticSearch - 좋긴 한데, 잘써야 하는 패키지이다. <br>
정해져있는 데이터들만 제공하거나, 한달치만 제공하겠다. 바운더리가 되어 있으면 좋다.<br>
좋다고 해서 우리회사 로그 전부 때려넣고 분석하면 큰일난다. <br>
왜냐면 시스템적인 Require 가 되게 많기 때문이다.<br>
모여져있는 데이터를 넣어가지고 분석하겠다? 하지마라.<br>
인덱싱 막 하고 그러면 빨간불이 켜지면(시스템 신호등 상태 나쁨) 시스템 늘려줘야 하고,<br>
넣는데 3-4일 걸리는.. 뭐 다 넣고나면 빠르다 (비용은 많이듬)<br>

**Logstash, Kafka, Tableau**<br>
ElasticSearch에 데이터를 넣어주는게 Logstash<br>
이따가 실습할때 Logstash가지고 한다. 데이터 수집하는데 좋아서.<br>
일반회사에서 수집하는거는 Kafka<br>
대시보드로 Visualization 하는것으로 뜨고있는것은 Tableau<br>

<br>

모든 솔루션 회사들이 클라우드 서비스를 한다. <br>
거기다 데이터를 올려주고, Visualization해서 볼수 있게 해준다.<br>
그 중에서 AWS사용해서 분석을 해보는 데이터 파이프라인 아키텍쳐 이다.<br>

분석이라는게, 폭이 커서 잘 생각해보면, Visualization해서 할수도 있고, 머신러닝이나 딥러닝을 돌려서 분석할수도 있다.<br>
근데 우리는 지금 그냥 파이프라인을 하는것이기 때문에 분석에 대해 깊게 하지는 않겠다.<br>
Tableau와 우리가 수집한 데이터와 어떻게 연계하는지, 이것만 되면 사실 어떤 툴하고 연계하는것은 어렵지 않다. <br>
각 단계에 대해 깊게 들어가지는 않겠다.<br>
Spark가 가장 중요하다. 전처리, 저장, 조회, 분석하는 툴! 이걸 많이 하려고 한다.<br>
보통 데이터를 어떻게 쉽게 저장을 하고, 저장된 로우 데이터는 어떻게 전처리해서, 어떤 형식의 정형데이터로 저장하면 좋은가?<br>
Apache superset은 airbnb에서 Visualization을 위한 툴인데, 오픈한지 오래 안된걸로 알고 있다.<br>

우선은 수집쪽을 Kafka를 가지고 AWS상에서 실습을 할것이다. <br>
클라우드 서비스는 다음주에 해볼것이다.<br>

<br>
<hr/>
<br>

#### 데이터 생성

보통 분석하는게 이런것이다, 메뉴중에서 어느 메뉴를 처음 아주 쉽게 클릭을 하고, 어떻게 들어갔는지 경로에 대한 분석.<br>
요즘 AB테스트 많이 하는데, 보통 2개 안하고 4,5개씩 한다. 케이스를 보고 적용을 한다.<br>
웹, 앱에서 발생하는 이벤트.<br>
DAU(하루 방문자) 5만명 중에서 원룸쪽으로 3만명 왔다. 그러면 어떻게 보여줘야 할지.<br>
아파트쪽으로 들어온 사람들을 선별해서 푸시를 보낼까 말까.<br>
KPI 지수(어떤 메뉴를 많이 클릭하면 도움이 되냐)<br>
어느 채널로 마케팅 했을때 전화가 많이 오느냐 (마이너스인지 아닌지 지표를 봐야함)<br>
어느 요소에서 개인을 식별할 것인가.(앱이 가진 애드아이디)<br>

외부 협업체에서 발생하는 이벤트 <br>
  - 통화가 끝나자마자 거기서 이벤트를 날려서 추가되면 빠르게 수급할수 있는 시스템
  - 인터페이스는 회사가 클수록 많을것이다. 비씨카드는 얼마나 많을까.
  - 회사간의 비지니스는 비씨카드에 고객사가 많아서 속도가 느릴수밖에 없다는것을 고려해주지 않는다.
  - 수집하는 사람은 빨리 받아서 분석을 하고 싶기 때문이다.
  - 파이프라인을 구성할때는 어떻게 하면 빨리 그걸 연결할 것인지 생각해야한다.

보통 Post 방식으로 쏘고, (web, ios, android)<br>
보통 Json 형식으로 쏩니다. (반정형, 유연성 좋음)<br>
tcp/ip는 예전 방식. 잘 안맞으면 까다로운 부분이 있지만 확실하긴 하다.<br>

<br>

#### 수집

Kinesis stream : Kafka를 잘 베껴서 잘 녹여서 서비스<br>
쿼리스트링 Get방식 - 그대로 저장 가능하지만, json으로 변환하는 방식을 취해서 처리했다.<br>
반정형 Json형태로 정의<br>
비정형 데이터는 데이터 양이 커서 당연히 압축을 해야한다.기본이다. 텍스트는 70~90%의 압축률이 나온다.<br>
빅데이터 나오면서 gzip, tar 말고도 여러가지 압축형태들이 많이 나온다.<br>
빅데이터는 속도에 또 포커스에 되어있긴 하다. 그렇지만 어쨌든 무조건 압축.<br>
Firehose(형태 변환해서 여러 채널에 저장): 압축이 안돼서 서비스를 쓰려고 했다가 빼버렸다.<br>
Redshift : 데이터 웨어하우스 툴(RDS sql + Hadoop 분산). 근데 비싸다.<br>

생성->수집->전처리->저장->분석->결과저장<br>
어떤 형태로 어디에, 왜 거기에 저장해야 하는지?<br>
이 데이터를 사용하는 사람의 수준에 맞게 제공을 해줘야 한다.<br>

<br>
<hr/>
<br>

#### 클라우드 서비스를 사용하면 비싸지 않나요?

추천하는 인스턴스는 1시간에 0.08불. 메모리 30기가 SSD 80~100기가. <br>
퍼포먼스는 느리겠지만 10대를 10시간 써도 8불.<br>
EMR 쓰는데 과금이 더 붙긴 하지만, 그래도 10불밖에 안된다.<br>
10대면 하루에 1기가 쌓이는 데이터 분석하는데 아무런 지장이 없다.<br>

<br>
<hr/>
<br>

#### 용어

**온프레미스**<br>
데이터센터 상에 서버를 올려서 운영하는 방식<br>
대략 클라우드 컴퓨팅의 반대 개념<br>
운영하면서 예상치 못한일이 항상 발생한다. 직접 대비하든 발생시 처리하든 해줘야한다.<br>

**메타데이터**<br>
데이터에 대한 데이터, 다른 데이터를 설명해주는 데이터<br>
크든 작든 관리를 해주시는게 좋다.<br>

**ETL**<br>
추출(Extract), 변환(Transform), 적재(Load)<br>
여러군데에 있는 소스데이터를 추출<br>
적절한 포맷으로 전처리 과정을 통해서 변환<br>
최종 대상으로 데이터를 적재<br>
여러 군데에 있는 소스데이터를 Visualization 하기 위해 쓰는 툴<br>
근데 스파크를 쓰면 똑같다.<br>

**Producer**<br>
메시지가 발생하는 부분. 생산하는 주체.<br>

**Message queue**<br>
발생한 메시지를 보통 큐에다 담는다. (카프카, 키네시스)<br>
큐 없다면 처리할때까지 사용자가 기다려야 하니까.<br>

**Consumer**<br>
메시지를 쓰는 놈들. 소비하는 주체.<br>
큐들이 쌓이면 갖다 써야 하니까.<br>

**클러스터**<br>
여러개의 컴퓨터들이 연결되어, 그 집합(클러스터)을 한대처럼 쓰는것이다.<br>
EMR자체도 클러스터이고, 우리가 쓸려고 하는게 다 클러스터이다.<br>
클러스터를 스케일 업/ 다운, 아웃/인 을 할수 있다.<br>
* 스케일 업 / 다운 : 고성능 장비로<br>
* 스케일 아웃 / 인 : 대수를 늘리는것<br>

**Kafka**<br>
분산 메시지 처리 시스템<br>
RabbitMQ는 메모리. 근데 Kafka는 디스크 사용해서 데이터 영속성 보장함<br>
기존 대비 TPS 성능 우수<br>
카프카 클러스터가 데이터를 가지고 있는데, 특정 사이즈만큼 데이터를 잘라서 땡겨 쓸수 있다. (Kafka stream)<br>
몇기가를 가지고 있을건지, 몇시간을 유지할건지 설정할 수 있다.<br>
장애나면 일주일치 가지고 있어야 하지 않을까 싶었지만 잘 안나더라 하루로 바꿨다.<br>

**리전(Region)**<br>
각 나라마다의 데이터센터다.<br>
같은 리전의 데이터 전송은 무료<br>
다른 리전으로 나갈때만 받아요. (들어오면 자기네 시스템 쓸거니까 안받나봄)<br>

<br>
<hr/>
<br>

<!--
#### 데이터 파이프라인을 위한 AWS 패키지

**EC2**<br>
인스턴스(가상 컴퓨터 한대)
리소스를 필요할때 올렸다가 내리는 프로그래밍을 한다
볼륨레벨
in 되는 ip를 설정 해줄수 있고.
공인 아이피 하나 매핑을 해줌 (쓸땐 무료, 안쓰면 돈냄)
태그 : 리소스를 관장하고 관리하기 위한 메타데이터

**S3**<br>
웹하드
무제한, 에러율이 0에 가깝다
루트폴더를 버킷이라고 하고요
거기에 있는 파일을 오브젝트라고 함
파일들을 키라고 함(?)
데이터 레이크 구성을 s3에다 하는거구요
주제별로 폴더별로 쌓으면 되는것

**RDS**<br>
삭제가 쉬워서 조심해야 함
리소스 다 날려버릴수도 있음
복구는 쉽지만, 장애 상황 데이터는 유실할수밖에 없겠죠

**Api Gateway**<br>
마이크로 시스템 나오면서 나온 개념
aws 서비스에 들어갈수 있는 문중에 하나입니다 (나트도 그중에 하나)
서버리스 패키지 중에 하나입니다
인터넷에서 접근할수 있는

**CloudWatch**<br>
모니터링, 알림 할수 있는 툴
약간 로그보기가 안좋긴 한데
-->
